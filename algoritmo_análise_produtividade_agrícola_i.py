# -*- coding: utf-8 -*-
"""Algoritmo_Análise Produtividade agrícola I.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wFJr5bCJtTchATJtUSpPh5lww6vq5hlg

1º) Análise exploratória dos dados.
V
ariável	Tipo	Tratamento Sugerido	Justificativa
Poluentes (PM10, PM2.5, O₃, NOx)	Contínua	Interpolação linear sazonal	Preserva padrões anuais de poluição
Meteorológicas (Temp, Umidade, Chuva)	Contínua	Média móvel de 7 dias	Suaviza variações climáticas abruptas
Queimadas	Contínua discreta	Preencher com 0 (ausência de registro)	Assume que não houve queimadas não registradas
Rendimento Agrícola	Alvo	Excluir linhas	Evita viés na variável dependente
"""

# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

# Read the Excel file
df = pd.read_excel('Base de dados_Oficial.xlsx', sheet_name='Planilha1')

# Print the column names to verify the exact spelling
print(df.columns)

# 1. Tratamento específico para queimadas
# Check the output of df.columns to find the correct column name for 'Número de queimadas'
# Replace 'Número de queimadas' below with the exact column name if it's different
df['Número de queimadas em Piracicaba'] = df['Número de queimadas em Piracicaba'].fillna(0)

# 2. Interpolação sazonal para poluentes
poluentes = ['Material Particulado (PM10) µg/m3', 'Material Particulado (PM2.5) µg/m3', 'Ozônio (O₃) µg/m3',
             'Óxidos de nitrogênio (NO) µg/m3', 'Óxidos de nitrogênio (NO₂)', 'Óxidos de nitrogênio (NOₓ)']
for col in poluentes:
    # Add a check to ensure the column exists before attempting to interpolate
    if col in df.columns:
        df[col] = df.groupby('Mês')[col].transform(
            lambda x: x.interpolate(method='linear', limit_direction='both')
        )
    else:
        print(f"Warning: Column '{col}' not found in DataFrame.")


# 3. Imputação iterativa para variáveis climáticas
climaticas = ['Temperatura média do ar (°C)', 'Umidade relativa do ar (%)', 'Precipitação (mm)','Velocidade e direção do vento (m/s, °)']
# Add a check to ensure all climatic columns exist before imputation
if all(col in df.columns for col in climaticas):
    imp = IterativeImputer(max_iter=10, random_state=42)
    df[climaticas] = imp.fit_transform(df[climaticas])
else:
    missing_climatic_cols = [col for col in climaticas if col not in df.columns]
    print(f"Warning: The following climatic columns were not found in the DataFrame: {missing_climatic_cols}")


# 4. Remoção de linhas sem rendimento
rendimento_col = 'Rendimento médio (kg/ha) Piracicaba'
if rendimento_col in df.columns:
    df.dropna(subset=[rendimento_col], inplace=True)
else:
    print(f"Warning: Rendimento column '{rendimento_col}' not found in DataFrame. Skipping dropna.")

# Import necessary libraries for this cell (ensure they are imported if not already in previous cells)
import pandas as pd
import numpy as np # Often useful for numerical operations, even if not directly used in fillna here

# --- Nova Etapa: Imputação de 'Número de dias com chuva' pela Mediana Mensal ---

# A coluna 'Número de dias com chuva' foi a única que manteve valores nulos após os tratamentos anteriores.
# Vamos verificar se ela existe antes de tentar imputar.
if 'Número de dias com chuva' in df.columns:
    print(f"\nImputando 'Número de dias com chuva' usando a mediana mensal. Valores ausentes antes: {df['Número de dias com chuva'].isnull().sum()}")

    # Calcular a mediana do 'Número de dias com chuva' para cada mês
    # e preencher os NaNs do respectivo mês.
    # É importante que a coluna 'Mês' esteja presente e consistente.
    df['Número de dias com chuva'] = df.groupby('Mês')['Número de dias com chuva'].transform(
        lambda x: x.fillna(x.median())
    )

    # Verifique se ainda restam NaNs após a imputação sazonal.
    # Isso pode acontecer se um mês específico tiver *todos* os seus valores como NaN.
    if df['Número de dias com chuva'].isnull().any():
        remaining_nulls = df['Número de dias com chuva'].isnull().sum()
        print(f"Atenção: Após imputação pela mediana mensal, ainda restam {remaining_nulls} valores nulos em 'Número de dias com chuva'. Preenchendo com a mediana geral como fallback.")
        df['Número de dias com chuva'] = df['Número de dias com chuva'].fillna(df['Número de dias com chuva'].median())

    # Arredondar para inteiros e converter para int
    # Agora que tratamos os NaNs, esta conversão deve ser segura.
    df['Número de dias com chuva'] = df['Número de dias com chuva'].round().astype(int)
    print("'Número de dias com chuva' imputado com a mediana mensal e convertido para tipo inteiro.")
else:
    print("Atenção: Coluna 'Número de dias com chuva' não encontrada no DataFrame. Ignorando imputação.")


# Display info and head of the DataFrame after all cleaning steps
print("\nDataFrame Info após todas as etapas de limpeza:")
df.info()

print("\nDataFrame Head após todas as etapas de limpeza:")
print(df.head())

# Check for remaining null values
print("\nNúmero de valores nulos após todas as etapas de limpeza:")
print(df.isnull().sum())

import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd # Ensure pandas is imported if running this cell independently

# Matriz de correlação
corr = df.select_dtypes(include=np.number).corr()
plt.figure(figsize=(20, 16))  # Ajuste o tamanho conforme necessário
sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=".2f", annot_kws={"rotation": 45})
plt.title('Correlação entre Variáveis', pad=20)
plt.tight_layout()  # Evita cortar rótulos
plt.show()

print(corr)

df.info()

# Análise rápida de número de queimadas x produtividade
sns.scatterplot(x='Número de queimadas em Piracicaba', y='Rendimento médio (kg/ha) Piracicaba', data=df)
plt.title('Relação entre Número de Queimadas e Produtividade')
plt.show()
corr_q_p = df['Número de queimadas em Piracicaba'].corr(df['Rendimento médio (kg/ha) Piracicaba'])
print(f"Correlação entre Número de Queimadas e Produtividade: {corr_q_p}")

#Análise de máximos e mínimos nos dados

import matplotlib.pyplot as plt
import seaborn as sns

# Lista de variáveis de interesse
variaveis = [
    'Número de queimadas em Piracicaba', # Corrected column name
    'Material Particulado (PM10) µg/m3', # Assuming 'PM10' corresponds to this column
    'Material Particulado (PM2.5) µg/m3', # Assuming 'PM2.5' corresponds to this column
    'Óxidos de nitrogênio (NO) µg/m3',    # Assuming 'NO' corresponds to this column
    'Óxidos de nitrogênio (NO₂)',       # Assuming 'NO₂' corresponds to this column
    'Óxidos de nitrogênio (NOₓ)',       # Assuming 'NOₓ' corresponds to this column
    'Ozônio (O₃) µg/m3',               # Assuming 'O₃' corresponds to this column
    'Temperatura média do ar (°C)',
    'Umidade relativa do ar (%)',
    'Número de dias com chuva',
    'Precipitação (mm)',
    'Rendimento médio (kg/ha) Piracicaba'
]

# Configuração dos gráficos
plt.figure(figsize=(15, 20))
for i, var in enumerate(variaveis, 1):
    plt.subplot(6, 2, i)
    # Add a check to ensure the column exists before plotting
    if var in df.columns:
        sns.boxplot(data=df, y=var, x='Ano', order=[
            '2013', '2014', '2015', '2016', '2017', '2018',
            '2019', '2020', '2021', '2022', '2023'
        ])
        plt.title(f'Boxplot de {var} por Mês')
        plt.xticks(rotation=45)
    else:
        print(f"Warning: Column '{var}' not found in DataFrame. Skipping boxplot.")

plt.tight_layout()
plt.show()

"""**Uso da análise de componentes principais(PCA)**

Interpretação:

PC1: Se concentra em poluentes (PM10, PM2.5, NOx) e queimadas (alta correlação positiva) vs. clima (temperatura, umidade, chuva – carga negativa).

PC2: Pode capturar padrões opostos (ex.: O₃ vs. NO₂).

Aplicação: Use os 2-3 primeiros PCs em modelos de regressão para evitar multicolinearidade.



"""

df_2 = df
df_2

print("Colunas antes da seleção de variáveis:")
print(df.columns)

# Lista de colunas a serem removidas devido à alta multicolinearidade
# Mantemos PM2.5 e NOx como representantes
columns_to_drop_multicollinearity = [
    'Material Particulado (PM10) µg/m3',
    'Óxidos de nitrogênio (NO) µg/m3',
    'Óxidos de nitrogênio (NO₂)'
]

# Remover as colunas do DataFrame
# Usamos errors='ignore' para evitar erro caso alguma coluna já tenha sido removida ou não exista
df_cleaned_multicollinearity = df.drop(columns=columns_to_drop_multicollinearity, errors='ignore')

print("\nColunas após a seleção de variáveis para tratar multicolinearidade:")
print(df_cleaned_multicollinearity.columns)

print("\nDataFrame Info após seleção de variáveis:")
df_cleaned_multicollinearity.info()

print("\nPrimeiras 5 linhas do DataFrame após seleção de variáveis:")
print(df_cleaned_multicollinearity.head())

"""**RANDOM FOREST - PRIMEIRO MODELO**
Entendendo o Random Forest Regressor
O Random Forest (Floresta Aleatória) é um algoritmo de aprendizado de máquina pertencente à categoria de modelos de ensemble (conjunto). O termo "ensemble" significa que ele combina as previsões de múltiplos modelos mais simples (chamados "estimadores base") para produzir uma previsão final mais robusta e precisa. No caso do Random Forest, os estimadores base são árvores de decisão.

Imagine que você tem um problema complexo e pede a opinião de vários especialistas, em vez de apenas um. Se esses especialistas forem diversos e cometerem erros de forma independente, a média de suas opiniões provavelmente será mais precisa do que a opinião de qualquer um deles isoladamente. O Random Forest opera de forma similar.

Como funciona (de forma simplificada):

Criação de Múltiplas Árvores: Em vez de construir uma única árvore de decisão, o Random Forest constrói um grande número de árvores de decisão independentes (a "floresta").
Aleatoriedade na Construção das Árvores: Para garantir que as árvores sejam diversas (evitando que todas deem as mesmas respostas), duas formas de aleatoriedade são introduzidas:
Bagging (Bootstrap Aggregating): Cada árvore é treinada em uma subamostra aleatória dos seus dados originais (com reposição). Isso significa que algumas observações podem ser usadas múltiplas vezes e outras podem não ser usadas em uma determinada árvore.
Seleção Aleatória de Features: Em cada nó de uma árvore, apenas um subconjunto aleatório das suas variáveis preditoras (features) é considerado para a divisão dos dados. Isso força as árvores a explorarem diferentes combinações de variáveis e evita que uma única feature muito forte domine todas as árvores.
Agregação das Previsões: Para fazer uma previsão:
Se for um problema de regressão (como o seu, onde prevemos um valor numérico como a produtividade), o Random Forest coleta as previsões de todas as árvores individuais e calcula a média dessas previsões para obter o resultado final.
Se for um problema de classificação, ele geralmente usa a "votação" da maioria.
O que Pretendemos com o Random Forest no Seu TCC:
Com o Random Forest, pretendemos alcançar os seguintes objetivos:

Capturar Relações Não Lineares e Interações Complexas: Dada a baixa correlação linear que observamos entre algumas variáveis atmosféricas/climáticas e o rendimento, o Random Forest é ideal para identificar e modelar relações mais complexas e não lineares que os modelos de regressão linear simples não conseguiriam. Ele pode descobrir, por exemplo, que a produtividade é afetada por uma combinação específica de alta temperatura e baixa precipitação, algo que uma análise linear dificilmente revelaria.
Robustez a Outliers e Multicolinearidade: O algoritmo é menos sensível a outliers e à multicolinearidade entre as variáveis preditoras (como vimos com os poluentes) em comparação com modelos lineares, o que o torna mais estável e confiável para os seus dados.
Melhorar o Poder Preditivo: Ao combinar várias árvores, o Random Forest tende a ter um desempenho preditivo superior ao de uma única árvore de decisão e, muitas vezes, supera modelos lineares em datasets com relações complexas.
Avaliar a Importância das Features: O Random Forest pode nos fornecer uma medida de "importância da feature". Isso nos dirá quais variáveis preditoras foram mais influentes na tomada de decisão das árvores e, consequentemente, na previsão do rendimento. Essa informação é valiosa para entender quais fatores ambientais e atmosféricos são os mais relevantes para a produtividade agrícola em Piracicaba.
Fornecer um Baseline Robusto: Ele servirá como um modelo de referência sólido para comparar com as Redes Neurais Artificiais (RNAs), conforme seu projeto original.
"""

# Import necessary libraries
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd # Import pandas again, just to be sure it's available in this cell context

# Assumindo que 'df_cleaned_multicollinearity' é o DataFrame resultante da célula anterior.
# Se você o reatribuiu para 'df' na célula anterior, use 'df' aqui.
# Por clareza, vou manter df_cleaned_multicollinearity como o nome do DataFrame de entrada.

# 1. Preparação dos dados para o modelo
# A variável alvo (o que queremos prever)
target_variable = 'Rendimento médio (kg/ha) Piracicaba'

# As variáveis preditoras (features)
# Excluímos as variáveis de identificação e as variáveis que não são preditoras diretas.
# ATENÇÃO: Se você alterou a lista de colunas na etapa de tratamento de multicolinearidade,
# ou se alguma dessas colunas não existia, ajuste esta lista.
features_to_exclude = [
    target_variable,
    'Rendimento médio (kg/ha) SP', # Variável relacionada, mas não é preditora direta do rendimento final de Piracicaba
    'FCi (SP-Piraricaba)',         # É um fator de ajuste, não uma característica ambiental/atmosférica
    'Cidade',                      # Categoria única para Piracicaba neste dataset
    'Ano',                         # Pode ser usada como feature, mas por enquanto vamos manter as ambientais/atmosféricas
    'Mês'                          # Categoria de mês. Para Random Forest, podemos lidar com ela de algumas formas.
                                   # Por enquanto, vamos excluí-la e usar apenas as numéricas.
                                   # No futuro, podemos convertê-la para variáveis dummy ou cyclical features.
]

# Garantir que apenas as colunas existentes no DataFrame final sejam excluídas
actual_features_to_exclude = [col for col in features_to_exclude if col in df_cleaned_multicollinearity.columns]

# Selecionar apenas colunas numéricas como features iniciais,
# e depois remover as 'features_to_exclude' específicas.
numeric_cols = df_cleaned_multicollinearity.select_dtypes(include=np.number).columns.tolist()
features = [col for col in numeric_cols if col not in actual_features_to_exclude]

# Exibir as features que serão usadas
print("\nVariáveis preditoras (Features) para o modelo Random Forest:")
print(features)

X = df_cleaned_multicollinearity[features]
y = df_cleaned_multicollinearity[target_variable]

# Verifique se há NaNs restantes em X ou y antes de treinar
if X.isnull().sum().sum() > 0 or y.isnull().sum() > 0:
    print("\nAVISO: Existem valores NaN em X ou y antes do treinamento do modelo. Verifique a etapa de limpeza de dados.")
    print("NaNs em X:\n", X.isnull().sum()[X.isnull().sum() > 0])
    print("NaNs em y:\n", y.isnull().sum())


# 2. Divisão dos dados em conjuntos de treinamento e teste
# 80% treino, 20% teste.
# random_state para reprodutibilidade.
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"\nTamanho do conjunto de treinamento (X_train): {X_train.shape}")
print(f"Tamanho do conjunto de teste (X_test): {X_test.shape}")

# 3. Inicialização e Treinamento do Modelo Random Forest Regressor
# n_estimators: número de árvores na floresta (um bom ponto de partida é 100)
# random_state: para reprodutibilidade
# n_jobs=-1: usa todos os núcleos disponíveis da CPU para acelerar o treinamento
model_rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)

print("\nTreinando o modelo Random Forest...")
model_rf.fit(X_train, y_train)
print("Treinamento concluído.")

# 4. Avaliação do Modelo
y_pred_rf = model_rf.predict(X_test)

rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))
r2_rf = r2_score(y_test, y_pred_rf)

print(f"\n--- Avaliação do Modelo Random Forest ---")
print(f"RMSE (Root Mean Squared Error): {rmse_rf:.2f}")
print(f"R² (Coeficiente de Determinação): {r2_rf:.2f}")

# 5. Análise de Importância das Features
# Ajuda a entender quais variáveis o modelo considerou mais relevantes
importances = model_rf.feature_importances_
feature_names = X.columns
forest_importances = pd.Series(importances, index=feature_names).sort_values(ascending=False)

fig, ax = plt.subplots(figsize=(12, 8))
forest_importances.plot.bar(ax=ax)
ax.set_title("Importância das Features (Random Forest)")
ax.set_ylabel("Média da Redução de Impureza")
plt.tight_layout()
plt.show()

# 6. Visualização das Previsões vs. Valores Reais (Opcional, mas muito útil para entender o desempenho)
plt.figure(figsize=(10, 6))
sns.scatterplot(x=y_test, y=y_pred_rf, alpha=0.6) # alpha para transparência em caso de muitos pontos
plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2) # Linha de 45 graus para referência perfeita
plt.xlabel("Valores Reais de Rendimento (kg/ha)")
plt.ylabel("Valores Previstos de Rendimento (kg/ha)")
plt.title("Previsões do Random Forest vs. Valores Reais")
plt.grid(True)
plt.show()

df['Rendimento médio (kg/ha) Piracicaba'].describe()
df['Rendimento médio (kg/ha) Piracicaba'].value_counts()
plt.figure(figsize=(15, 7)); df.set_index(pd.to_datetime(df['Ano'].astype(str) + '-' + df['Mês'].astype(str).map({'Janeiro': '01', 'Fevereiro': '02', 'Março': '03', 'Abril': '04', 'Maio': '05', 'Junho': '06', 'Julho': '07', 'Agosto': '08', 'Setembro': '09', 'Outubro': '10', 'Novembro': '11', 'Dezembro': '12'}))).plot(y='Rendimento médio (kg/ha) Piracicaba'); plt.title('Série Temporal do Rendimento Médio em Piracicaba'); plt.show()

df['Rendimento médio (kg/ha) Piracicaba'].describe()
df['Rendimento médio (kg/ha) Piracicaba'].value_counts()

# Import necessary libraries (garanta que pandas e numpy já estejam importados)
import pandas as pd
import numpy as np

# Assumindo que 'df_cleaned_multicollinearity' é o DataFrame resultante da célula anterior.

print("DataFrame original para agregação (primeiras linhas):")
print(df_cleaned_multicollinearity.head())
print("\nColunas do DataFrame original:", df_cleaned_multicollinearity.columns.tolist())

# Definir as colunas que serão agregadas e como serão agregadas
# Para variáveis como temperatura, umidade, poluentes, geralmente usamos a média
# Para precipitação, número de dias com chuva, queimadas, geralmente usamos a soma
# O 'FCi' e 'Rendimento médio SP' também serão agregados se estiverem no DF e forem relevantes anualmente.

# Identificar colunas para agregação
# Excluímos 'Cidade', 'Mês', e a própria coluna de rendimento que se comporta anualmente.
# 'Ano' será a chave para o agrupamento.
# Vamos pegar apenas as features numéricas que sobram após a limpeza anterior,
# e tratá-las de forma apropriada.

# Selecionar as colunas que são FEATURES e NUMÉRICAS do dataframe limpo
numeric_features_to_aggregate = [
    'Número de queimadas em Piracicaba',
    'Material Particulado (PM2.5) µg/m3',
    'Óxidos de nitrogênio (NOₓ)',
    'Ozônio (O₃) µg/m3',
    'Temperatura média do ar (°C)',
    'Umidade relativa do ar (%)',
    'Velocidade e direção do vento (m/s, °)',
    'Número de dias com chuva',
    'Precipitação (mm)'
    # Adicione aqui outras colunas numéricas que são features e você deseja agregar
    # Ex: 'Rendimento médio (kg/ha) SP', 'FCi (SP-Piraricaba)'
    # Se você quiser mantê-las como features preditoras anuais, deve agregá-las também.
    # Como FCi e Rendimento SP são relacionados ao cálculo do target, vamos agregá-los se eles forem usados como features
]

# Adicionar FCi e Rendimento SP para agregação se forem considerados features
if 'Rendimento médio (kg/ha) SP' in df_cleaned_multicollinearity.columns:
    numeric_features_to_aggregate.append('Rendimento médio (kg/ha) SP')
if 'FCi (SP-Piraricaba)' in df_cleaned_multicollinearity.columns:
    numeric_features_to_aggregate.append('FCi (SP-Piraricaba)')

# Filtrar apenas as colunas que realmente existem no DataFrame
numeric_features_to_aggregate = [col for col in numeric_features_to_aggregate if col in df_cleaned_multicollinearity.columns]

# Criar um dicionário de agregação: média para a maioria, soma para precipitação/queimadas/dias_chuva
aggregation_dict = {}
for col in numeric_features_to_aggregate:
    if 'Precipitação' in col or 'chuva' in col or 'queimadas' in col:
        aggregation_dict[col] = 'sum'
    else:
        aggregation_dict[col] = 'mean'

print("\nDicionário de agregação:", aggregation_dict)

# Agrupar por 'Ano' e aplicar as agregações
df_annual = df_cleaned_multicollinearity.groupby('Ano').agg(aggregation_dict)

# Re-adicionar a coluna de Rendimento de Piracicaba (assumindo que já é um valor anual por ano)
# Como o rendimento já parece ser anual (comportamento de degraus), vamos pegar o primeiro valor de cada ano.
# No entanto, para ser mais robusto se houver variações *dentro* dos blocos anuais,
# o ideal é pegar a média (ou o último valor, dependendo do que ele representa).
# Dado o seu gráfico, a média anual parece ser a representação mais fiel do valor "anualizado"
# que ele já representa em seu dataset mensal.
annual_rendimento = df_cleaned_multicollinearity.groupby('Ano')['Rendimento médio (kg/ha) Piracicaba'].mean()
df_annual['Rendimento médio (kg/ha) Piracicaba'] = annual_rendimento


# Resetar o índice para 'Ano' voltar a ser uma coluna, se desejar
df_annual.reset_index(inplace=True)


print("\nDataFrame Anual Agregado (primeiras 5 linhas):")
print(df_annual.head())

print("\nDataFrame Info do Agregado Anual:")
df_annual.info()

print("\nValores Nulos no DataFrame Anual Agregado:")
print(df_annual.isnull().sum())

# Agora você pode usar df_annual para a próxima etapa de modelagem
# O número de linhas será bem menor (um por ano), mas os dados estarão consistentes.

# Import necessary libraries (garanta que todas as libs anteriores estejam importadas)
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split, KFold # Adicionando KFold
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd

# Assumindo que 'df_annual' é o DataFrame resultante da célula anterior

# 1. Preparação dos dados para o modelo
target_variable = 'Rendimento médio (kg/ha) Piracicaba'

# As variáveis preditoras (features) para o modelo anual
# Desta vez, EXCLUINDO 'Rendimento médio (kg/ha) SP' e 'FCi (SP-Piraricaba)'
features_to_exclude_final = [
    target_variable,
    'Ano',
    'Rendimento médio (kg/ha) SP',
    'FCi (SP-Piraricaba)'
]

# Garantir que apenas as colunas existentes no DataFrame sejam excluídas
actual_features_to_exclude_final = [col for col in features_to_exclude_final if col in df_annual.columns]

# Selecionar todas as colunas numéricas e depois remover as excluídas
numeric_cols_annual = df_annual.select_dtypes(include=np.number).columns.tolist()
features = [col for col in numeric_cols_annual if col not in actual_features_to_exclude_final]

# Exibir as features que serão usadas
print("\nVariáveis preditoras (Features) para o modelo Random Forest (agregado anual, sem FCi/Rendimento SP):")
print(features)

X = df_annual[features]
y = df_annual[target_variable]

# Verifique se há NaNs restantes em X ou y (deve ser 0 agora)
if X.isnull().sum().sum() > 0 or y.isnull().sum() > 0:
    print("\nAVISO: Existem valores NaN em X ou y antes do treinamento do modelo. Verifique a etapa de limpeza de dados.")
    print("NaNs em X:\n", X.isnull().sum()[X.isnull().sum() > 0])
    print("NaNs em y:\n", y.isnull().sum())

# --- MUDANÇA IMPORTANTE: Usando Validação Cruzada K-Fold devido ao pequeno tamanho do dataset ---
# A divisão 80/20 resultou em X_test com apenas 3 amostras, o que não é robusto.
# K-Fold dividirá o dataset em 'k' partes, e em cada iteração, uma parte será teste e o restante treino.
# Com 11 amostras, um KFold de 5 (k=5) significa que em cada iteração:
# (11 / 5) = ~2-3 amostras para teste e ~8-9 para treino.
# As métricas serão uma média de 5 execuções.
kf = KFold(n_splits=5, shuffle=True, random_state=42) # shuffle para embaralhar os dados

rmse_scores = []
r2_scores = []
feature_importances_accumulated = np.zeros(len(features))

print(f"\nIniciando Treinamento e Avaliação com K-Fold Cross Validation (k=5)...")
fold = 1
for train_index, test_index in kf.split(X):
    print(f"\n--- Fold {fold} ---")
    X_train_fold, X_test_fold = X.iloc[train_index], X.iloc[test_index]
    y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]

    # 3. Inicialização e Treinamento do Modelo Random Forest Regressor
    model_rf_kfold = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)
    model_rf_kfold.fit(X_train_fold, y_train_fold)

    # 4. Avaliação do Modelo para o Fold Atual
    y_pred_rf_kfold = model_rf_kfold.predict(X_test_fold)

    rmse_fold = np.sqrt(mean_squared_error(y_test_fold, y_pred_rf_kfold))
    r2_fold = r2_score(y_test_fold, y_pred_rf_kfold)

    rmse_scores.append(rmse_fold)
    r2_scores.append(r2_fold)

    print(f"RMSE (Fold {fold}): {rmse_fold:.2f}")
    print(f"R² (Fold {fold}): {r2_fold:.2f}")

    # Acumular importância das features para média
    feature_importances_accumulated += model_rf_kfold.feature_importances_
    fold += 1

print(f"\n--- Avaliação Média do Modelo Random Forest (K-Fold - Dados Anuais) ---")
print(f"RMSE Médio: {np.mean(rmse_scores):.2f} (± {np.std(rmse_scores):.2f})")
print(f"R² Médio: {np.mean(r2_scores):.2f} (± {np.std(r2_scores):.2f})")

# 5. Análise de Importância das Features (Média sobre os folds)
feature_importances_mean = feature_importances_accumulated / kf.get_n_splits()
forest_importances_mean = pd.Series(feature_importances_mean, index=X.columns).sort_values(ascending=False)

fig, ax = plt.subplots(figsize=(12, 8))
forest_importances_mean.plot.bar(ax=ax)
ax.set_title("Importância Média das Features (Random Forest - Dados Anuais - K-Fold)")
ax.set_ylabel("Média da Redução de Impureza")
plt.tight_layout()
plt.show()

# 6. Visualização das Previsões vs. Valores Reais (Opcional, mais complexo com K-Fold para um único gráfico)
# Para este, teríamos que armazenar todas as previsões e reais de cada fold e plotar juntos,
# mas para um dataset tão pequeno, a média das métricas já é mais informativa.
# Se for fazer, seria algo como:
# all_y_test = np.concatenate([y.iloc[test_index] for train_index, test_index in kf.split(X)])
# all_y_pred = np.concatenate([model_rf_kfold.predict(X.iloc[test_index]) for train_index, test_index in kf.split(X)])

# No entanto, com apenas 11 pontos, um scatterplot geral pode ser menos útil do que
# a análise das métricas médias e importância das features.

# Import necessary libraries
from sklearn.linear_model import LinearRegression, Lasso, Ridge
from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np
import pandas as pd # Certifique-se de que pandas está importado

# Assumindo que 'df_annual' é o DataFrame resultante da célula anterior

# 1. Preparação dos dados para o modelo
target_variable = 'Rendimento médio (kg/ha) Piracicaba'

# As mesmas variáveis preditoras que usamos para o Random Forest (sem FCi/Rendimento SP)
features_to_exclude_final = [
    target_variable,
    'Ano',
    'Rendimento médio (kg/ha) SP',
    'FCi (SP-Piraricaba)'
]
actual_features_to_exclude_final = [col for col in features_to_exclude_final if col in df_annual.columns]
numeric_cols_annual = df_annual.select_dtypes(include=np.number).columns.tolist()
features = [col for col in numeric_cols_annual if col not in actual_features_to_exclude_final]

X = df_annual[features]
y = df_annual[target_variable]

print("\n--- Modelos de Regressão Linear (RLR, Lasso, Ridge) ---")
print("Features utilizadas:", features)
print(f"Tamanho do dataset: X={X.shape}, y={y.shape}")

# Configuração da Validação Cruzada K-Fold
kf = KFold(n_splits=5, shuffle=True, random_state=42)

# Dicionário para armazenar modelos e seus parâmetros (alpha para Lasso/Ridge)
models = {
    'Linear Regression (RLR)': LinearRegression(),
    'Lasso': Lasso(alpha=1.0, random_state=42),  # alpha é um hiperparâmetro de regularização. Começaremos com 1.0
    'Ridge': Ridge(alpha=1.0, random_state=42)   # alpha é um hiperparâmetro de regularização. Começaremos com 1.0
}

# Loop para treinar e avaliar cada modelo
for model_name, model in models.items():
    rmse_scores = []
    r2_scores = []

    print(f"\nIniciando avaliação para: {model_name}...")

    for train_index, test_index in kf.split(X):
        X_train_fold, X_test_fold = X.iloc[train_index], X.iloc[test_index]
        y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]

        model.fit(X_train_fold, y_train_fold)
        y_pred_fold = model.predict(X_test_fold)

        rmse_scores.append(np.sqrt(mean_squared_error(y_test_fold, y_pred_fold)))
        r2_scores.append(r2_score(y_test_fold, y_pred_fold))

    print(f"--- Avaliação Média do Modelo {model_name} (K-Fold) ---")
    print(f"RMSE Médio: {np.mean(rmse_scores):.2f} (± {np.std(rmse_scores):.2f})")
    print(f"R² Médio: {np.mean(r2_scores):.2f} (± {np.std(r2_scores):.2f})")

    # Opcional: Coeficientes do modelo linear (apenas para RLR e Ridge, Lasso pode ter zeros)
    if hasattr(model, 'coef_'):
        print("Coeficientes (apenas o primeiro fold ou último modelo treinado):")
        for feature, coef in zip(features, model.coef_):
            print(f"  {feature}: {coef:.4f}")

import statsmodels.api as sm
import pandas as pd
import numpy as np

# --- Assumindo que 'df' é o seu DataFrame final para Milho - Rendimento (kg/ha) ---
# Ou seja, 'df' aqui seria equivalente ao df_milho_rendimento_final do nosso código unificado,
# mas que no seu contexto de células quebradas, ainda pode conter 'Cidade' e 'Mês'.

# Certificar que todas as colunas são numéricas para o modelo
# Identificar colunas não numéricas que precisam ser excluídas
non_numeric_cols = df.select_dtypes(exclude=np.number).columns.tolist()

# Definindo a variável alvo
target_col = 'Rendimento médio (kg/ha) Piracicaba'

# As features para o modelo OLS: todas as numéricas, excluindo o target, 'Ano', 'Rendimento médio (kg/ha) SP', e 'FCi (SP-Piraricaba)'.
# E também excluindo quaisquer colunas não numéricas remanescentes.
features_to_exclude_from_model = [
    target_col,
    'Ano', # 'Ano' é uma ID/índice, não uma feature para significância linear direta (embora possa ser usada em outros contextos)
    'Rendimento médio (kg/ha) SP', # Excluída por ser relacionada ao cálculo do target original
    'FCi (SP-Piraricaba)' # Excluída pelo mesmo motivo
]

# Combinar as colunas não numéricas com as features a excluir
all_cols_to_drop_from_X = list(set(non_numeric_cols + features_to_exclude_from_model))

# Criar o DataFrame X contendo apenas as features numéricas desejadas
X = df.drop(columns=all_cols_to_drop_from_X, errors='ignore')
y = df[target_col]

# Verificar se X ainda contém colunas não numéricas (para debug)
if X.select_dtypes(exclude=np.number).empty is False:
    print("\nAVISO: X ainda contém colunas não numéricas após a exclusão. Verifique:", X.select_dtypes(exclude=np.number).columns.tolist())
    raise ValueError("X contém colunas não numéricas. Ajuste a exclusão de colunas.")


print("\n--- Análise de Significância Estatística para Regressão Linear (Milho - Rendimento kg/ha) ---")
print("Features utilizadas:", X.columns.tolist()) # Verifique se 'Cidade' e 'Mês' não estão aqui

# Adicionar uma constante (intercepto) ao modelo
X_sm = sm.add_constant(X)

# Criar e ajustar o modelo OLS
model_sm = sm.OLS(y, X_sm)
results_sm = model_sm.fit()

# Imprimir o sumário completo
print(results_sm.summary())

# Import necessary libraries
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression # Usaremos LinearRegression sobre as features polinomiais
from sklearn.model_selection import KFold
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np
import pandas as pd # Certifique-se de que pandas está importado

# Assumindo que 'df_annual' é o DataFrame resultante da célula anterior

# 1. Preparação dos dados para o modelo
target_variable = 'Rendimento médio (kg/ha) Piracicaba'

# As mesmas variáveis preditoras que usamos anteriormente
features_to_exclude_final = [
    target_variable,
    'Ano',
    'Rendimento médio (kg/ha) SP',
    'FCi (SP-Piraricaba)'
]
actual_features_to_exclude_final = [col for col in features_to_exclude_final if col in df_annual.columns]
numeric_cols_annual = df_annual.select_dtypes(include=np.number).columns.tolist()
features = [col for col in numeric_cols_annual if col not in actual_features_to_exclude_final]

X = df_annual[features]
y = df_annual[target_variable]

print("\n--- Modelo de Regressão Polinomial Multivariada (Grau 2) ---")
print("Features utilizadas para a transformação polinomial:", features)
print(f"Tamanho do dataset: X={X.shape}, y={y.shape}")

# Configuração da Validação Cruzada K-Fold
kf = KFold(n_splits=5, shuffle=True, random_state=42)

rmse_scores = []
r2_scores = []

print("\nIniciando avaliação para: Regressão Polinomial (Grau 2)...")

for fold, (train_index, test_index) in enumerate(kf.split(X)):
    print(f"\n--- Fold {fold + 1} ---")
    X_train_fold, X_test_fold = X.iloc[train_index], X.iloc[test_index]
    y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]

    # Criar features polinomiais
    # include_bias=False para evitar coluna de 1s, pois LinearRegression já adiciona
    poly = PolynomialFeatures(degree=2, include_bias=False)
    X_train_poly = poly.fit_transform(X_train_fold)
    X_test_poly = poly.transform(X_test_fold)

    # Nomes das novas features polinomiais para depuração (opcional)
    # poly_feature_names = poly.get_feature_names_out(features)
    # print("Número de features polinomiais criadas:", len(poly_feature_names))

    # Treinar um modelo de Regressão Linear sobre as features polinomiais
    model_poly = LinearRegression()
    model_poly.fit(X_train_poly, y_train_fold)

    # Fazer previsões
    y_pred_fold = model_poly.predict(X_test_poly)

    # Avaliar
    rmse_scores.append(np.sqrt(mean_squared_error(y_test_fold, y_pred_fold)))
    r2_scores.append(r2_score(y_test_fold, y_pred_fold))

    print(f"RMSE (Fold {fold + 1}): {rmse_scores[-1]:.2f}")
    print(f"R² (Fold {fold + 1}): {r2_scores[-1]:.2f}")

print(f"\n--- Avaliação Média do Modelo Regressão Polinomial (Grau 2) (K-Fold) ---")
print(f"RMSE Médio: {np.mean(rmse_scores):.2f} (± {np.std(rmse_scores):.2f})")
print(f"R² Médio: {np.mean(r2_scores):.2f} (± {np.std(r2_scores):.2f})")